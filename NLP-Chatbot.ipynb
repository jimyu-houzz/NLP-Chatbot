{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"liyu7088_COMP5046_Ass1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MGHoy6KpQDfZ","colab_type":"text"},"source":["# COMP5046 Assignment 1\n","LiangChun Yu   \n","SID: 480317999   \n","Unicode: liyu7088\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qTf21j_oQIiD","colab_type":"text"},"source":["# Readme\n","\n","\n","Each step is commented through the process."]},{"cell_type":"markdown","metadata":{"id":"iXbQohXLKSgO","colab_type":"text"},"source":["***Visualising the comparison of different results is a good way to justify your decision.***"]},{"cell_type":"markdown","metadata":{"id":"34DVNKgqQY21","colab_type":"text"},"source":["# 1 - Data Preprocessing (Personality chat datasets)"]},{"cell_type":"markdown","metadata":{"id":"7cWUxAQrGlq6","colab_type":"text"},"source":["## 1.1. Download Dataset (Personality chat datasets)"]},{"cell_type":"code","metadata":{"id":"U7C4snIcNl22","colab_type":"code","outputId":"3ec7d0b8-7371-4b2c-f019-f3432df8a2a4","executionInfo":{"status":"ok","timestamp":1559458483785,"user_tz":-600,"elapsed":12275,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Impporting packages, and the raw data for the assignment\n","import nltk\n","nltk.download('punkt')\n","import pandas as pd\n","import re\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import tensorflow as tf\n","\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","\n","id = '1slWYfNpATxnHg_11_XMWrzHOdxW8iHcH'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('qna_chitchat_the_professional.tsv') \n","\n","id = '16KDzEPGGJYuS97EZ_kU82y37y0qWVwb9'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('qna_chitchat_the_friend.tsv') \n","\n","id = '1d08B982HlHHGZ_-ZnS04Dt30HZhFPsdc'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('qna_chitchat_the_comic.tsv') \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l9gBSgBCQh24","colab_type":"text"},"source":["## 1.2. Preprocess data (Personality chat datasets)"]},{"cell_type":"markdown","metadata":{"id":"8RdKI8E2KRwe","colab_type":"text"},"source":["###A function is written to pre-process three of the data files. Through the function, we get the unique word list, sequence data, maximum input words(for further padding needs), and num_dic.###\n","\n","###Note that only the question is preprocessed, decapitalisation and remove punctuations. Answers remain the same since we're doing a N to 1 model.###\n","\n","### After running the function three times, we store all the data in separate dictionaries, which is easier to call in further process.###"]},{"cell_type":"code","metadata":{"id":"emyl1lWxGr12","colab_type":"code","colab":{}},"source":["# Please comment your code\n","\n","# read and preprocess input data, return unique words list\n","def preprocess_data(file_name):\n","  file_name = file_name\n","  df = pd.read_csv(file_name, sep=\"\\t\")\n","  \n","  seq_data = []\n","  whole_words = []\n","  unique_words = []\n","  max_input_words = 0\n","  df = pd.read_csv(file_name, sep=\"\\t\")\n","  \n","# Read in text one row at a time, remove punctuation and decpitalize\n","# text for all questions.\n","  for index, row in df.iterrows():\n","    question = row['Question']\n","    answer = row['Answer']\n","    seq_data.append([question, answer])\n","    \n","    question = re.sub(r'[^\\w\\s]','', question.lower())\n","    tokenized_q = nltk.tokenize.word_tokenize(question)\n","    \n","    whole_words += tokenized_q\n","    whole_words.append(answer)\n","    max_input_words = max(len(tokenized_q), max_input_words)\n","  unique_words = list(set(whole_words))\n","  unique_words.append('_B_')\n","  unique_words.append('_E_')\n","  unique_words.append('_P_')\n","  unique_words.append('_U_')\n","  unique_words.sort()\n","# Sort words in a order so everytime we generate, we have the same order\n","  num_dic = {n:i for i,n in enumerate(unique_words)}\n","  return unique_words, seq_data, max_input_words, num_dic\n","\n","d_comic, seq_comic, max_word_comic, num_dic_comic = preprocess_data('qna_chitchat_the_comic.tsv')\n","d_friend, seq_friend, max_word_friend, num_dic_friend = preprocess_data('qna_chitchat_the_friend.tsv')\n","d_professional, seq_professional, max_word_professional, num_dic_professional = preprocess_data('qna_chitchat_the_professional.tsv')\n","\n","\n","### Store all the genearated data in dictionaries, making it easier to call later\n","num_dic = {'comic':num_dic_comic, 'friend':num_dic_friend, 'professional':num_dic_professional}\n","# Store all num dics in a nested dictionary.\n","max_input_words = max(max_word_comic, max_word_friend, max_word_professional)\n","# To simplify the code, we just take the max words of the three models(though they're all the same)\n","dic_len_models = {'comic':len(num_dic['comic']), 'friend':len(num_dic['friend']), 'professional':len(num_dic['professional'])}\n","# And we have a dic of the dic_len of three modesl\n","unique_word_dic = {'comic':d_comic,'friend':d_friend, 'professional':d_professional}\n","# Store different unique_word list in a dict\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIu_lkJwQ55g","colab_type":"text"},"source":["# 2 - Model Implementation"]},{"cell_type":"markdown","metadata":{"id":"daDvAftceIvr","colab_type":"text"},"source":["## 2.1. Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"lbzm-NWBTmM-","colab_type":"text"},"source":["*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *\n","\n","###We use Word2Vec SkipGram for our word embeddings. From lectures and reserach, skipgram normally performs better than CBOW, and Word2Vec embeddings model can be generated through a one-liner function, and model trained in advanve, which is convenient for the assignment.###"]},{"cell_type":"markdown","metadata":{"id":"it6I1_K7HTub","colab_type":"text"},"source":["### 2.1.1. Download Dataset for Word Embeddings\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4Op66omXKVHa","colab_type":"text"},"source":["The same dataset is used, making it easier to process and build models."]},{"cell_type":"markdown","metadata":{"id":"GXgFpxIgl-_G","colab_type":"text"},"source":["### 2.1.2. Data Preprocessing for Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"qJrVHGYSmYMg","colab_type":"text"},"source":[" We decapitalize, remove puctuations, remove numbers, then tokenize word, to generate a tokens list for the word embedding."]},{"cell_type":"code","metadata":{"id":"3LByzHLiNinu","colab_type":"code","colab":{}},"source":["# Please comment your code\n","\n","# Generate token list for word embeddings.\n","def process_data_embedding(seq_data):\n","  seq_data = seq_data\n","  \n","  normalized_text = []\n","  tok_list = []\n","  for seq in seq_data:\n","    question = seq[0]\n","    answer = seq[1]\n","# preprocess data, and generate tokens. Do not tokenize answers.    \n","    question = re.sub(r\"[^a-z0-9]+\", \" \", question.lower())\n","    question = re.sub(r'[^\\w\\s]','',question)\n","    tokens = word_tokenize(question)\n","    tokens.append(answer)\n","    tok_list.append(tokens)\n","  \n","  tok_list.append(['_B_','_E_','_P_','_U_']) \n","      \n","  return tok_list"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qhAgWf_AmbZ8","colab_type":"text"},"source":["### 2.1.3. Build Word Embeddings Model"]},{"cell_type":"markdown","metadata":{"id":"AJ8rU7JbiBVS","colab_type":"text"},"source":["*You are required to describe how hyperparameters were decided with justification of your decision.*\n","\n","###We keep most of the parameters same(SkipGram), only changing min_count to 1, since we want to keep all words in the word2vec vocabulary. Vector size of 100 causes the training to be slower, but it does causes larger costs in the further seq2seq training process, so we keep the size as 100.###"]},{"cell_type":"code","metadata":{"id":"TVPuwWgvNjOU","colab_type":"code","colab":{}},"source":["# Word2Vec models are stored in the models dictionary\n","\n","models = {}\n","\n","models['comic'] = Word2Vec(sentences=process_data_embedding(seq_comic), size=100, window=5, min_count=1, workers=4, sg=1)\n","models['friend'] = Word2Vec(sentences=process_data_embedding(seq_friend), size=100, window=5, min_count=1, workers=4, sg=1)\n","models['professional'] = Word2Vec(sentences=process_data_embedding(seq_professional), size=100, window=2, min_count=1, workers=4, sg=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wq5apFL9-J9O","colab_type":"code","outputId":"35567e62-d981-4ada-e2af-92fc94b4f51e","executionInfo":{"status":"error","timestamp":1559459129630,"user_tz":-600,"elapsed":774,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":266}},"source":["a = models['comic']\n","print(len(a.wv.vocab))\n","print(a.wv.vocab['hi'])\n","print(type(a.wv.vocab['hi']))\n","print(a.wv.vocab['hi'].shape)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["617\n","Vocab(count:6, index:125, sample_int:4294967296)\n","<class 'gensim.models.keyedvectors.Vocab'>\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-a4f544f86ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'shape'"]}]},{"cell_type":"markdown","metadata":{"id":"LNys5HOdISK-","colab_type":"text"},"source":["### 2.1.4. Train Word Embeddings Model"]},{"cell_type":"code","metadata":{"id":"Ae8i7Z2kIef-","colab_type":"code","outputId":"4ff2efec-5947-45c8-de69-245da0c30e40","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Please comment your code\n","\n","# Word2vec models already trained."],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(45701, 68240)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"uMCv3YI1IfUo","colab_type":"text"},"source":["### 2.1.5. Save Word Embeddings Model"]},{"cell_type":"code","metadata":{"id":"3OwicNPkIqd1","colab_type":"code","outputId":"12992ac5-8568-4986-aa82-b85be2acac41","executionInfo":{"status":"ok","timestamp":1556473472525,"user_tz":-600,"elapsed":3116,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# Please comment your code\n","# Upload model to google drive, and storing the model ids in model_id\n","from gensim.test.utils import common_texts, get_tmpfile\n","\n","model_id = {}\n","folder_id = \"1YNUp5NkhlojOYio279f6DnVA71jBDUkP\"\n","# store in a specific folder of google drive\n","for key in models:\n","  path = get_tmpfile(\"word2vec.model\")\n","  models[key].save(\"word2vec.model\")\n","  model_name = key \n","\n","  uploaded = drive.CreateFile({'title': model_name, 'parents':[{\n","      'kind':'drive#fileLink','id':folder_id}]})\n","  uploaded.SetContentFile('word2vec.model')\n","  uploaded.Upload()\n","  model_id[key] = uploaded.get('id')\n","  print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","print(model_id)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Uploaded file with ID 1bUZaUs8Z4xc31xHl7R8vAzgSqNUOASfT\n","Uploaded file with ID 1uV5vF9THBZKv6yT3rui6BYnkj5KvkBx9\n","Uploaded file with ID 1HRzTl5E4D-LNYqZ1OLUw6MONKAwevhsB\n","{'comic': '1bUZaUs8Z4xc31xHl7R8vAzgSqNUOASfT', 'friend': '1uV5vF9THBZKv6yT3rui6BYnkj5KvkBx9', 'professional': '1HRzTl5E4D-LNYqZ1OLUw6MONKAwevhsB'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yn16xrDrIs8B","colab_type":"text"},"source":["### 2.1.6. Load Word Embeddings Model"]},{"cell_type":"code","metadata":{"id":"-IebpYFsIvgh","colab_type":"code","outputId":"eeeee9e9-b033-4df5-a18f-42a535226566","executionInfo":{"status":"ok","timestamp":1556473511314,"user_tz":-600,"elapsed":5321,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["# Call stored file ids, and download the models, store then in a new dict\n","download_models = {'comic':'','friend':'','professional':''}\n","for key in download_models:  \n","  id = model_id[key]\n","  downloaded = drive.CreateFile({'id':id}) \n","  downloaded.GetContentFile(key) \n","  download_models[key] = Word2Vec.load(key)\n","print(download_models)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'comic': <gensim.models.word2vec.Word2Vec object at 0x7f59a2b0db70>, 'friend': <gensim.models.word2vec.Word2Vec object at 0x7f59a17e57f0>, 'professional': <gensim.models.word2vec.Word2Vec object at 0x7f59a13244a8>}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tlCeWT8eeLnd","colab_type":"text"},"source":["## 2.2. Seq2Seq model"]},{"cell_type":"markdown","metadata":{"id":"fwA-NN3EJ4Ig","colab_type":"text"},"source":["### 2.2.1. Apply/Import Word Embedding Model"]},{"cell_type":"markdown","metadata":{"id":"UAMJrxx-iOVn","colab_type":"text"},"source":["We call the trainied word embeddings model downloaded above."]},{"cell_type":"markdown","metadata":{"id":"DpYCL17JKZxl","colab_type":"text"},"source":["### 2.2.2. Build Seq2Seq Model"]},{"cell_type":"markdown","metadata":{"id":"R204UIyDKhZ4","colab_type":"text"},"source":["\n","###We have global variables generated in the very first preprocess step.In this step we get the word2vec vectors for the input and outputbatch, target batch on the other hand is a num_dic series of numbers.Inside the following functions, we add paddings for questions, making every input batch the same window size. And we mark the beggining and end of the data of each sequence('_B_', '_E_'). '_U_' is added for words not in the word2vec vocabulary.###\n","\n","###Hyperparameters are remained mostly same from tutorials, we change number of epochs to 2500 since it is quite time consuming. n-class here is 100, which is the size of the word2vec vector. The session is also saved and uploaded to google drive in this step.###\n","\n","### We train three models separately in this step, since errors keep occuring while buiding models within a function.###"]},{"cell_type":"code","metadata":{"id":"13eCtR_SLUG6","colab_type":"code","colab":{}},"source":["# Please comment your code\n","\n","# We have seq_comic, seq_friend, seq_professional three sequence datas\n","# max_input_words is a global variable\n","# dic_len_models stores three dic_len of three modesl\n","def get_vectors_q(sentence, model):\n","  sentence = sentence\n","  model_local = model # the model where we retrieve the vectors\n","  \n","   # add paddings \n","  tokenized_sentence = sentence.split()\n","  diff = max_input_words - len(tokenized_sentence)  \n","  for x in range(diff):\n","    tokenized_sentence.append('_P_')\n","  \n","  # get vectors of each token\n","  ids = []\n","  for tok in tokenized_sentence:\n","# check if token in word embedding vocabulary\n","# then add the vector into ids    \n","    if tok in model_local.wv.vocab:\n","      ids.append(model_local[tok])\n","    else:\n","      ids.append(model_local['_U_'])\n","    \n","  return ids\n","\n","\n","def get_vectors_a(sentence, model):\n","  model_local = model\n","  ids = []\n","  if sentence in model_local.wv.vocab:\n","    ids.append(model_local[sentence])\n","  else:\n","    ids.append(model_local['_U_'])\n","  \n","  return ids\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hBIRk5B2zbws","colab_type":"code","colab":{}},"source":["def make_batch(seq_data, model_name):\n","  model = download_models[model_name]\n","  num_dic_local = num_dic[model_name]\n","  \n","  input_batch = []\n","  output_batch = []\n","  target_batch = []\n","  for seq in seq_data:\n","    input_batch.append(get_vectors_q(seq[0], model))\n","    \n","    output_data = []\n","    output_data.append(model['_B_'])\n","    output_data += get_vectors_a(seq[1], model)    \n","    output_batch += [output_data]\n","    \n","    target = []\n","    if seq[1] in num_dic_local:\n","      target.append(num_dic_local[seq[1]])\n","    else:\n","      target.append(num_dic_local['_U_'])\n","    target.append(num_dic_local['_E_'])\n","    target_batch += [target]\n","  \n","  return input_batch, output_batch, target_batch\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BaOiaGRLW7R","colab_type":"text"},"source":["### 2.2.3. Train Seq2Seq Model"]},{"cell_type":"code","metadata":{"id":"CN9lPQtENASc","colab_type":"code","outputId":"75e4a375-0782-4525-af21-e48f4373a426","executionInfo":{"status":"ok","timestamp":1556473770420,"user_tz":-600,"elapsed":208991,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":682}},"source":["# Build, train, and save model. Separately for three personalities.\n","\n","# Build model for 'comic'\n","dic_len = dic_len_models['comic']\n","\n","learning_rate = 0.002\n","n_hidden = 128\n","\n","n_class = dic_len\n","n_input = 100 # the shape of the word2vec vector\n","\n","### Neural Network Model\n","tf.reset_default_graph()\n","\n","# encoder/decoder shape = [batch size, time steps, input size]\n","enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n","dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n","\n","# target shape = [batch size, time steps]\n","targets = tf.placeholder(tf.int64, [None, None])\n","\n","\n","# Encoder Cell\n","with tf.variable_scope('encode'):\n","    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n","\n","    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n","                                            dtype=tf.float32)\n","# Decoder Cell\n","with tf.variable_scope('decode'):\n","    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n","\n","    # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n","    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n","                                            initial_state=enc_states,\n","                                            dtype=tf.float32)\n","\n","seq2_model = tf.layers.dense(outputs, n_class, activation=None)\n","cost = tf.reduce_mean(\n","            tf.nn.sparse_softmax_cross_entropy_with_logits(\n","                logits=seq2_model, labels=targets))\n","\n","optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n","\n","\n","# Save session\n","saver = tf.train.Saver() \n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# Generate a batch data\n","input_batch, output_batch, target_batch = make_batch(seq_comic, 'comic')\n","\n","total_epoch = 2500\n","\n","for epoch in range(total_epoch):\n","    _, loss = sess.run([optimizer, cost],\n","                       feed_dict={enc_input: input_batch,\n","                                  dec_input: output_batch,\n","                                  targets: target_batch})\n","    if epoch % 100 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1),\n","              'cost =', '{:.6f}'.format(loss))\n","\n","print('Epoch:', '%04d' % (epoch + 1),\n","      'cost =', '{:.6f}'.format(loss))\n","print('Training completed')\n","saver.save(sess, 'comic_final')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0001 cost = 6.439291\n","Epoch: 0101 cost = 2.269004\n","Epoch: 0201 cost = 2.253289\n","Epoch: 0301 cost = 2.243768\n","Epoch: 0401 cost = 2.246928\n","Epoch: 0501 cost = 2.238682\n","Epoch: 0601 cost = 2.241033\n","Epoch: 0701 cost = 2.232227\n","Epoch: 0801 cost = 2.239636\n","Epoch: 0901 cost = 2.241529\n","Epoch: 1001 cost = 2.239633\n","Epoch: 1101 cost = 2.240237\n","Epoch: 1201 cost = 2.237710\n","Epoch: 1301 cost = 2.244757\n","Epoch: 1401 cost = 2.242025\n","Epoch: 1501 cost = 2.236592\n","Epoch: 1601 cost = 2.242176\n","Epoch: 1701 cost = 2.243438\n","Epoch: 1801 cost = 2.235209\n","Epoch: 1901 cost = 2.238647\n","Epoch: 2001 cost = 2.238959\n","Epoch: 2101 cost = 2.237427\n","Epoch: 2201 cost = 1.993019\n","Epoch: 2301 cost = 1.738505\n","Epoch: 2401 cost = 1.552091\n","Epoch: 2500 cost = 1.467875\n","Training completed\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'comic_final'"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"QjoHufwqcvol","colab_type":"code","outputId":"80b12566-89ec-4caf-903a-e9eb2af5e6a6","executionInfo":{"status":"ok","timestamp":1556473787010,"user_tz":-600,"elapsed":4002,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# upload comic seq model\n","\n","folder_id = \"1YNUp5NkhlojOYio279f6DnVA71jBDUkP\"\n","\n","\n","uploaded = drive.CreateFile({'title': 'checkpoint', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./checkpoint')\n","uploaded.Upload()\n","comic_checkpoint_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'comic_meta', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./comic_final.meta')\n","uploaded.Upload()\n","comic_meta_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'comic_index', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./comic_final.index')\n","uploaded.Upload()\n","comic_index_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'comic_data', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./comic_final.data-00000-of-00001')\n","uploaded.Upload()\n","comic_data_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Uploaded file with ID 1ckhsS8AOc12jSQJCn6WSWrrM1-KVXbuf\n","Uploaded file with ID 1UZE7mm3Ij4RlFZSczFpFikkpa8scLdrB\n","Uploaded file with ID 1GVxACQ-lD8WLB9J06qYhJbmG_0_vteKm\n","Uploaded file with ID 1S_co064QT3-iS-FV8J8_zsnrpR6DedQ-\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hwNLBMZTQOZQ","colab_type":"code","outputId":"3076a1ee-7838-4bdc-a02f-c7eaed13f9f7","executionInfo":{"status":"ok","timestamp":1556474070961,"user_tz":-600,"elapsed":210267,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":682}},"source":["# Build \"friend\" seq model\n","\n","dic_len = dic_len_models['friend']\n","\n","learning_rate = 0.002\n","n_hidden = 128\n","n_class = dic_len\n","n_input = 100 # the shape of the word2vec vector\n","\n","tf.reset_default_graph()\n","enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n","dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n","targets = tf.placeholder(tf.int64, [None, None])\n","\n","# Encoder Cell\n","with tf.variable_scope('encode'):\n","    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n","    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n","                                            dtype=tf.float32)\n","# Decoder Cell\n","with tf.variable_scope('decode'):\n","    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n","    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n","                                            initial_state=enc_states,\n","                                            dtype=tf.float32)\n","\n","seq2_model = tf.layers.dense(outputs, n_class, activation=None)\n","cost = tf.reduce_mean(\n","            tf.nn.sparse_softmax_cross_entropy_with_logits(\n","                logits=seq2_model, labels=targets))\n","\n","optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n","\n","\n","saver = tf.train.Saver()\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","input_batch, output_batch, target_batch = make_batch(seq_friend, 'friend')\n","total_epoch = 2500\n","\n","for epoch in range(total_epoch):\n","    _, loss = sess.run([optimizer, cost],\n","                       feed_dict={enc_input: input_batch,\n","                                  dec_input: output_batch,\n","                                  targets: target_batch})\n","    if epoch % 100 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1),\n","              'cost =', '{:.6f}'.format(loss))\n","\n","print('Epoch:', '%04d' % (epoch + 1),\n","      'cost =', '{:.6f}'.format(loss))\n","print('Training completed')\n","\n","saver.save(sess, 'friend_final')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0001 cost = 6.439257\n","Epoch: 0101 cost = 2.266796\n","Epoch: 0201 cost = 2.249656\n","Epoch: 0301 cost = 2.247743\n","Epoch: 0401 cost = 2.246700\n","Epoch: 0501 cost = 2.247783\n","Epoch: 0601 cost = 2.243129\n","Epoch: 0701 cost = 2.245524\n","Epoch: 0801 cost = 2.243223\n","Epoch: 0901 cost = 2.247496\n","Epoch: 1001 cost = 2.246047\n","Epoch: 1101 cost = 2.249654\n","Epoch: 1201 cost = 2.248470\n","Epoch: 1301 cost = 2.244512\n","Epoch: 1401 cost = 2.248358\n","Epoch: 1501 cost = 2.247425\n","Epoch: 1601 cost = 2.249988\n","Epoch: 1701 cost = 2.242080\n","Epoch: 1801 cost = 1.978019\n","Epoch: 1901 cost = 1.801824\n","Epoch: 2001 cost = 1.661260\n","Epoch: 2101 cost = 1.546105\n","Epoch: 2201 cost = 1.462118\n","Epoch: 2301 cost = 1.366052\n","Epoch: 2401 cost = 1.340990\n","Epoch: 2500 cost = 1.269823\n","Training completed\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'friend_final'"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"-2feNpG-LZx2","colab_type":"text"},"source":["### 2.2.4. Save Seq2Seq Model"]},{"cell_type":"code","metadata":{"id":"sflUAgV4L1o8","colab_type":"code","outputId":"06fcc2b5-b636-447b-98d0-5eb0d8f31be2","executionInfo":{"status":"ok","timestamp":1556474089801,"user_tz":-600,"elapsed":4224,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# upload seq model for friend\n","folder_id = \"1YNUp5NkhlojOYio279f6DnVA71jBDUkP\"\n"," \n","uploaded = drive.CreateFile({'title': 'friend_checkpoint', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./checkpoint')\n","uploaded.Upload()\n","friend_checkpoint_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'friend_meta', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./friend_final.meta')\n","uploaded.Upload()\n","friend_meta_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'friend_index', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./friend_final.index')\n","uploaded.Upload()\n","friend_index_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'friend_data', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./friend_final.data-00000-of-00001')\n","uploaded.Upload()\n","friend_data_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Uploaded file with ID 12MFZ7JWiENkuGBID6FmDjt0JiIfGC4eP\n","Uploaded file with ID 1mEI8h0XtGHugVIovh1PXGRn7CSi6vNqd\n","Uploaded file with ID 16cKUgd-XXN-He5Xqeq2yPJFtnDDPLI5e\n","Uploaded file with ID 1FvcXL6NB3giwxXlVwAUmexV1D3oFxtJ-\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4TiX97-Rj_yg","colab_type":"code","outputId":"3552ff76-2ab3-457f-d0b1-388b55256db0","executionInfo":{"status":"ok","timestamp":1556474417684,"user_tz":-600,"elapsed":211717,"user":{"displayName":"余亮均","photoUrl":"https://lh4.googleusercontent.com/-gis88yS3TpY/AAAAAAAAAAI/AAAAAAAAAt4/C_R0xTfBGZQ/s64/photo.jpg","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":682}},"source":["# Train Professional Seq Model\n","\n","dic_len = dic_len_models['professional']\n","\n","learning_rate = 0.002\n","n_hidden = 128\n","n_class = dic_len\n","n_input = 100 # the shape of the word2vec vector\n","\n","\n","tf.reset_default_graph()\n","enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n","dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n","targets = tf.placeholder(tf.int64, [None, None])\n","\n","# Encoder Cell\n","with tf.variable_scope('encode'):\n","    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n","\n","    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n","                                            dtype=tf.float32)\n","# Decoder Cell\n","with tf.variable_scope('decode'):\n","    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n","    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n","                                            initial_state=enc_states,\n","                                            dtype=tf.float32)\n","    \n","seq2_model = tf.layers.dense(outputs, n_class, activation=None)\n","cost = tf.reduce_mean(\n","            tf.nn.sparse_softmax_cross_entropy_with_logits(\n","                logits=seq2_model, labels=targets))\n","optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n","\n","saver = tf.train.Saver()\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","input_batch, output_batch, target_batch = make_batch(seq_professional, 'professional')\n","\n","total_epoch = 2500\n","for epoch in range(total_epoch):\n","    _, loss = sess.run([optimizer, cost],\n","                       feed_dict={enc_input: input_batch,\n","                                  dec_input: output_batch,\n","                                  targets: target_batch})\n","    if epoch % 100 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1),\n","              'cost =', '{:.6f}'.format(loss))\n","\n","print('Epoch:', '%04d' % (epoch + 1),\n","      'cost =', '{:.6f}'.format(loss))\n","print('Training completed')\n","\n","saver.save(sess, 'professional_final')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0001 cost = 6.442017\n","Epoch: 0101 cost = 2.273122\n","Epoch: 0201 cost = 2.253241\n","Epoch: 0301 cost = 2.243272\n","Epoch: 0401 cost = 2.243923\n","Epoch: 0501 cost = 2.238600\n","Epoch: 0601 cost = 2.242177\n","Epoch: 0701 cost = 2.243120\n","Epoch: 0801 cost = 2.243827\n","Epoch: 0901 cost = 2.237396\n","Epoch: 1001 cost = 2.244642\n","Epoch: 1101 cost = 2.242625\n","Epoch: 1201 cost = 2.240697\n","Epoch: 1301 cost = 2.241271\n","Epoch: 1401 cost = 2.242280\n","Epoch: 1501 cost = 2.241081\n","Epoch: 1601 cost = 2.240868\n","Epoch: 1701 cost = 2.237412\n","Epoch: 1801 cost = 2.242924\n","Epoch: 1901 cost = 2.245448\n","Epoch: 2001 cost = 2.236545\n","Epoch: 2101 cost = 2.239727\n","Epoch: 2201 cost = 2.237613\n","Epoch: 2301 cost = 2.236903\n","Epoch: 2401 cost = 2.239925\n","Epoch: 2500 cost = 2.226492\n","Training completed\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'professional_final'"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"qK89SqhIkj24","colab_type":"code","outputId":"d511da4a-bf6c-4074-da66-e2c1e8de93fe","executionInfo":{"status":"error","timestamp":1569546119194,"user_tz":-600,"elapsed":1311,"user":{"displayName":"余亮均","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfz6tWGfKfrl9RH0cT9FHj0vYV8mUVTu0nBPyH4Q=s64","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["# Save Model of Professional\n","folder_id = \"1YNUp5NkhlojOYio279f6DnVA71jBDUkP\"\n"," \n","uploaded = drive.CreateFile({'title': 'professional_checkpoint', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./checkpoint')\n","uploaded.Upload()\n","professional_checkpoint_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'professional_meta', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./professional_final.meta')\n","uploaded.Upload()\n","professional_meta_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'professional_index', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./professional_final.index')\n","uploaded.Upload()\n","professional_index_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","uploaded = drive.CreateFile({'title': 'professional_data', 'parents':[{\n","    'kind':'drive#fileLink','id':folder_id}]})\n","uploaded.SetContentFile('./professional_final.data-00000-of-00001')\n","uploaded.Upload()\n","professional_data_id = uploaded.get('id')\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cbeaa7609426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfolder_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1YNUp5NkhlojOYio279f6DnVA71jBDUkP\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m uploaded = drive.CreateFile({'title': 'professional_checkpoint', 'parents':[{\n\u001b[0m\u001b[1;32m      4\u001b[0m     'kind':'drive#fileLink','id':folder_id}]})\n\u001b[1;32m      5\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"4zFo6YppL6w3","colab_type":"text"},"source":["### 2.2.5. Load Seq2Seq Model"]},{"cell_type":"code","metadata":{"id":"OtNxLzDGMCan","colab_type":"code","colab":{}},"source":["# Function to download seq2seq model\n","\n","def download_seq_model(model_name):\n","  if model_name == 'friend':\n","    id = friend_checkpoint_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('friend_checkpoint')\n","\n","    id = friend_meta_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('friend_meta')\n","\n","    id = friend_index_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('friend_index')\n","\n","    id = friend_data_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('friend_data')\n","  elif model_name == 'comic':\n","    id = comic_checkpoint_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('checkpoint')\n","\n","    id = comic_meta_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('comic_meta')\n","\n","    id = comic_index_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('comic_index')\n","\n","    id = comic_data_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('comic_data')\n","  elif model_name == 'professional':\n","    id = professional_checkpoint_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('professional_checkpoint')\n","\n","    id = professional_meta_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('professional_meta')\n","\n","    id = professional_index_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('professional_index')\n","\n","    id = professional_data_id\n","    downloaded = drive.CreateFile({'id':id})\n","    downloaded.GetContentFile('professional_data')\n","  else:\n","    print('Please enter correct personality.')\n","       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvNA0DP_hYB9","colab_type":"code","colab":{}},"source":["# Get respond for the input text, we build the model again inside, so the variables can function\n","\n","def answer(sentence, model_name):\n","  unique_word = unique_word_dic[model_name]\n","  dic_len = dic_len_models[model_name]\n","  tf.reset_default_graph()\n","  with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    \n","    learning_rate = 0.002\n","    n_hidden = 128\n","    n_class = dic_len\n","    n_input = 100\n","\n","    enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n","    dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n","    targets = tf.placeholder(tf.int64, [None, None])\n","\n","    # Encoder Cell\n","    with tf.variable_scope('encode'):\n","        enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","        enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n","\n","        outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n","                                                dtype=tf.float32)\n","    # Decoder Cell\n","    with tf.variable_scope('decode'):\n","        dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","        dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n","\n","        # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n","        outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n","                                                initial_state=enc_states,\n","                                                dtype=tf.float32)\n","    model = tf.layers.dense(outputs, n_class, activation=None)\n","    cost = tf.reduce_mean(\n","                tf.nn.sparse_softmax_cross_entropy_with_logits(\n","                    logits=model, labels=targets))\n","    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n","    \n","    restore_name = './'+model_name+'_final'\n","    saver = tf.train.Saver()\n","    saver.restore(sess, restore_name)\n","    \n","    seq_data = [sentence, '_U_']\n","    input_batch, output_batch, target_batch = make_batch([seq_data],model_name)\n","    \n","    prediction = tf.argmax(model, 2)\n","\n","    result = sess.run(prediction,\n","                      feed_dict={enc_input: input_batch,\n","                                 dec_input: output_batch,\n","                                 targets: target_batch})\n","\n","    # convert index number to actual token \n","    #unique_word_list = unique_word\n","    decoded = [unique_word[i] for i in result[0]]\n","        \n","    # Remove anything after '_E_'        \n","    if \"_E_\" in decoded:\n","        end = decoded.index('_E_')\n","        translated = ' '.join(decoded[:end])\n","    else :\n","        translated = ' '.join(decoded[:])\n","    \n","    return translated\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4mpRpocePLN","colab_type":"text"},"source":["# 3 - Evaluation (Running chatbot)"]},{"cell_type":"markdown","metadata":{"id":"Q5k5heyHL7cV","colab_type":"text"},"source":["###In this step, we write start_chat, change_personality, and save_chat_log functions for the chatbot. Only by typing 'exit' can the user finish the chat. 'change' is used to change personality. The chat is also recorded in two separate lists, and will be written to a text file after the chat is terminated."]},{"cell_type":"markdown","metadata":{"id":"KEW1zMgVMREr","colab_type":"text"},"source":["## 3.1. Start chatting"]},{"cell_type":"code","metadata":{"id":"_UdcH3Hwiyh1","colab_type":"code","colab":{}},"source":["# Start Chat\n","\n","def start_chat():\n","  print(\"Chatbot: Welcome. Please choose a personality.\")  \n","  personality = change_personality()\n","\n","  temp = ''\n","  change_option = ['change', 'exit']\n","  type_list = []\n","  respond_list = []\n","  while temp not in change_option:\n","    word = input(\"You: \")\n","    temp = word.lower()\n","    respond = answer(word, personality)\n","    print(\"Chatbot: \"+ respond)\n","    type_list.append(word)\n","    respond_list.append(respond)\n","    \n","  return type_list, respond_list, temp\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P28Z1k36MZuo","colab_type":"text"},"source":["## 3.2. Change Personality"]},{"cell_type":"markdown","metadata":{"id":"U8OBtJfvMgL_","colab_type":"text"},"source":["*Explain how to change personality (What is the command for changing personality?). *"]},{"cell_type":"code","metadata":{"id":"wTLyQEeZMZ2f","colab_type":"code","colab":{}},"source":["# Please comment your code\n","def change_personality():\n","  print(\"\")\n","  print(\"Chatbot: Please choose a personality.\")  \n","  word = input(\"You: \").lower()\n","  #word = word.lower()\n","  person_list = ['comic', 'friend', 'professional']\n","  while word not in person_list:    \n","    print(\"Please type a correct personality.\")\n","    word = input(\"You: \").lower()\n","  print(\"Chatbot: Okay, let's chat!\")\n","  return word\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y50Ep8KKMZ99","colab_type":"text"},"source":["## 3.3. Save chat log"]},{"cell_type":"code","metadata":{"id":"bbZ6oOu6MaGJ","colab_type":"code","colab":{}},"source":["# Chat record written and saved to local directory.\n","from google.colab import files\n","\n","def save_chat_log(input_file, output_file):\n","  f = open(\"chat_log.txt\", \"w\")\n","  for i in range(len(input_file)):\n","    f.write(\"You: \"+input_file[i]+\"\\n\")\n","    f.write(\"Chatbot: \"+output_file[i]+\"\\n\")\n","  f.close()\n","  files.download('chat_log.txt')\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JISqR3jjMwwU","colab_type":"text"},"source":["## 3.4. End chatting"]},{"cell_type":"code","metadata":{"id":"nT_DeoHSMw49","colab_type":"code","colab":{}},"source":["# Please comment your code\n","\n","# When user types \"exit\", the chat is ended."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HpomO_3YNI5X","colab_type":"text"},"source":["## 3.5. Execute program"]},{"cell_type":"markdown","metadata":{"id":"cDkQJ9i_NH9D","colab_type":"text"},"source":["***Please make sure your program  is running properly.***\n","\n","***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"]},{"cell_type":"markdown","metadata":{"id":"_7J5hS_SOIUU","colab_type":"text"},"source":["### 3.5.1. Execute program - training mode"]},{"cell_type":"markdown","metadata":{"id":"_woLwuU3Mk3w","colab_type":"text"},"source":["*Please include lines to train the bot.*"]},{"cell_type":"code","metadata":{"id":"xhWYz7NQOfLV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"outputId":"80d9b477-4234-45e3-97c3-76818f462ed3","executionInfo":{"status":"error","timestamp":1569546104973,"user_tz":-600,"elapsed":1370,"user":{"displayName":"余亮均","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfz6tWGfKfrl9RH0cT9FHj0vYV8mUVTu0nBPyH4Q=s64","userId":"13433110493268860181"}}},"source":["# Download the seq_models\n","\n","download_seq_model('comic')\n","download_seq_model('friend')\n","download_seq_model('professional')\n","\n"],"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-cb5429b055c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownload_seq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'comic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdownload_seq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'friend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdownload_seq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'professional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'download_seq_model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"65cZTuQ_OeI7","colab_type":"text"},"source":["### 3.5.2. Execute program - chatting mode"]},{"cell_type":"markdown","metadata":{"id":"D7LrbcP_PKap","colab_type":"text"},"source":["*Please include lines to start chatting with the bot.*"]},{"cell_type":"code","metadata":{"id":"QVvzZsB7PbYf","colab_type":"code","outputId":"33ed131d-fac2-4c5e-db20-351442f3a74c","executionInfo":{"status":"error","timestamp":1569546080296,"user_tz":-600,"elapsed":1374,"user":{"displayName":"余亮均","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfz6tWGfKfrl9RH0cT9FHj0vYV8mUVTu0nBPyH4Q=s64","userId":"13433110493268860181"}},"colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["# To start chat, we call the start_chat function.\n","\n","\n","list_of_input = []\n","list_of_output = []\n","\n","type_list, respond_list, change = start_chat()\n","\n","# only 'change' or 'exit' will jump out of the loop\n","\n","while change == 'change':\n","  list_of_input += type_list\n","  list_of_output += respond_list\n","  type_list, respond_list, change = start_chat()\n","# the only way to exit loop is when change = 'exit'\n","\n","print(\"Chatbot: Thank you! See you again.\")\n","\n","save_chat_log(list_of_input, list_of_output)\n","\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ab25c5a829bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlist_of_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtype_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrespond_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# only 'change' or 'exit' will jump out of the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'start_chat' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"sfv8rWTKPzeb","colab_type":"text"},"source":["## Object Oriented Programming codes here"]},{"cell_type":"markdown","metadata":{"id":"TS23AjBRSZaX","colab_type":"text"},"source":["*If you have multiple classes use multiple code snippets to add them.*"]},{"cell_type":"code","metadata":{"id":"wSJJ4zRFQy1h","colab_type":"code","colab":{}},"source":["# If you used OOP style, use this sectioon"],"execution_count":0,"outputs":[]}]}